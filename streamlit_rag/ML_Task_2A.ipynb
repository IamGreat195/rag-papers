{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Do run all of these as a .py file using command prompt [streamlit run \"FILENAME\"]** it is presented as a notebook only to explain the intricacies of its code"
      ],
      "metadata": {
        "id": "W8DiluGt2se6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXVoOavy2NhN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So our research papers will be stored in the same directory with their actual titles, we should be able to map them to a easier title to remember and use in our code"
      ],
      "metadata": {
        "id": "VlhP_VZX3q2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll have to load in these pdfs, extract their text, split it into chunks and then store it in a vector store from which we can later load it in"
      ],
      "metadata": {
        "id": "j260y2j9389X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@st.cache_resource(show_spinner=False) # we are caching the vector store to avoid reloading it every time we click somewhere in the website\n",
        "def load_vector_store():\n",
        "    # File mapping[we are just mapping the file names to their titles so that we can use them later in the metadata of the documents]\n",
        "    filename_to_title = {\n",
        "        \"attention is all you need.pdf\": \"Attention Is All You Need\",\n",
        "        \"BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\": \"BERT\",\n",
        "        \"GPT-3 Language Models are Few-Shot Learners.pdf\": \"GPT-3\",\n",
        "        \"Contrastive Language-Image Pretraining with Knowledge Graphs.pdf\": \"CLIP\",\n",
        "        \"LLaMA Open and Efficient Foundation Language Models.pdf\": \"LLaMA\"\n",
        "    }\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) # we are splitting the text into chunks of 1000 characters with an overlap of 200 characters\n",
        "    document_chunks = []\n",
        "\n",
        "    def extract_text(pdf_path):\n",
        "        text = \"\"\n",
        "        with open(pdf_path, 'rb') as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file) # Read the PDF file\n",
        "            # Loop through each page in the PDF and extract text\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    for filename in os.listdir('research_papers'):\n",
        "        if filename.endswith('.pdf'):\n",
        "            pdf_path = os.path.join('research_papers', filename)\n",
        "            full_text = extract_text(pdf_path)\n",
        "            title = filename_to_title.get(filename, \"Unknown Paper\") # getting the corresponding title from the dictionary mapping\n",
        "            chunks = text_splitter.split_text(full_text)\n",
        "            for chunk in chunks:\n",
        "                document_chunks.append(Document(page_content=chunk, metadata={\"source\": title})) # we are creating a Document object for each chunk with the title as metadata\n",
        "\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2') # using a pre-trained sentence transformer model for embeddings\n",
        "\n",
        "    vector_store = Chroma.from_documents( # creating a vector store from the documents for efficient retrieval\n",
        "        documents=document_chunks,\n",
        "        embedding=embedding_model,\n",
        "        persist_directory=\"chroma_db\" # persist is needed to save the vector store to disk so that we can load it later without reprocessing the documents\n",
        "    )\n",
        "\n",
        "    return vector_store\n"
      ],
      "metadata": {
        "id": "65Ze4T1Q2qce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets set up langchain for analysis(it contains the various document chunks retrived via the questions and so on)"
      ],
      "metadata": {
        "id": "BTGJaZP65bG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "VxoO8dpj5E7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat model set up"
      ],
      "metadata": {
        "id": "s-ZhRQ0H6ekf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDfeVwwdJojhi6Ose_GSKg-Eb_g0zpNR48\"\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash', temperature=0) # i am using gemini-2.0-flash model for my chat model[temperature basically means how creative the model can be, lower means more ridgid answers]"
      ],
      "metadata": {
        "id": "XS5R2O4B5vW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For generating the answers to the queries we'll basically use reciprocal rank fusion method, which is basically a more advanced version of multi query where the intial query sent by the user is rephrased and split into 5 similar questions by the language model which are then used to retrieve similar looking document chunks from the vector store"
      ],
      "metadata": {
        "id": "IgWzzs307AFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "vector_store = load_vector_store() # we load the vector store from the cached function\n",
        "retriever = vector_store.as_retriever() # we create a retriever from the vector store to retrieve documents based on the generated queries\n",
        "\n",
        "\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (6 queries):\"\"\" # rag fusion template to generate multiple queries based on a single input query\n",
        "\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template) # we create a chat prompt template from the rag fusion template\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion # we pass the template into gemini, remove the text wrapper for chats and split the 5 questions formatting them with new lines\n",
        "    | ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "K2DCVqES8274"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heres where reciprocal rank fusion diverges from simple multi query method, the retrieved chunks are then ranked based on the RRF formula, higher rrf score meaning more relevant and then again for the final time we pass in the top ranked context chunks to the model with the original question and get the answer"
      ],
      "metadata": {
        "id": "mhVB1vWn8b_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RRF score = ∑ 1/k+r"
      ],
      "metadata": {
        "id": "D-2gHr3k9j6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where r is the rank and k is a constant"
      ],
      "metadata": {
        "id": "gBEVOCq598EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion # we are passing in the queries into the retriever(mapping the retriever object to each query) and the output of ranked documents is passed into the rrf function\n"
      ],
      "metadata": {
        "id": "Y_72tj__9_g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import streamlit as st"
      ],
      "metadata": {
        "id": "3oCaf_jFAnsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to define our final template that will get us all the ranked document chunks together to answer our original question"
      ],
      "metadata": {
        "id": "gUsTv1npA37V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\" # rag template to answer the question based on the context retrieved from the vector store\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "final_rag_chain = ( # we create a final rag chain that takes the context and question as input and returns the answer\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "4lvUHiF3A2vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have to package all these rag chains into a website model, we'll be using streamlit for it"
      ],
      "metadata": {
        "id": "w-HczNhJBB-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# streamlit app\n",
        "st.set_page_config(page_title=\"RAG Fusion QA\", page_icon=\"🔍\")\n",
        "st.title(\"🔍 Ask Questions About ML Papers\")\n",
        "\n",
        "question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "if question:\n",
        "    with st.spinner(\"Retrieving and answering...\"):\n",
        "        retrieval_chain_rag_fusion.invoke({\"question\": question}) # we are invoing the retrieval chain and getting the documents\n",
        "        answer = final_rag_chain.invoke({\"question\": question}) # we are then passing in the context through the retrieval rag chain and the question and getting the final answer\n",
        "        st.markdown(\"### Answer:\")\n",
        "        st.success(answer)"
      ],
      "metadata": {
        "id": "aUZYAi57BHo-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}